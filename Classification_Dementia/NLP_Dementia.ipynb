{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3a94556",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "df = pd.read_csv('Dementia.csv')\n",
    "df2 = pd.read_csv('Control.csv')\n",
    "\n",
    "df3 = pd.DataFrame()\n",
    "\n",
    "# Add the \"cookie test\" column from df to df3\n",
    "df3['cookie test'] = df['PAR']\n",
    "\n",
    "# Add the \"dementia\" column with value 1 for all rows in df3\n",
    "df3['dementia'] = 1\n",
    "\n",
    "# Copy \"Error Type 1\" and \"Error Type 2\" columns from df to df3\n",
    "df3['Error Type 1'] = df['Error Type 1']\n",
    "df3['Error Type 2'] = df['Error Type 2']\n",
    "df3['File'] = df['File']\n",
    "df3['Age'] = df['Age']\n",
    "df3['Gender'] = df['Gender']\n",
    "df3['MMSE'] = df['MMSE']\n",
    "\n",
    "# Create a copy of df2 with the same structure as df3\n",
    "df2_copy = pd.DataFrame({'cookie test': df2['PAR'], 'dementia': 0})\n",
    "\n",
    "# Copy \"Error Type 1\" and \"Error Type 2\" columns from df2 to df2_copy\n",
    "df2_copy['Error Type 1'] = df2['Error Type 1']\n",
    "df2_copy['Error Type 2'] = df2['Error Type 2']\n",
    "df2_copy['File'] = df2['File']\n",
    "df2_copy['Age'] = df2['Age']\n",
    "df2_copy['Gender'] = df2['Gender']\n",
    "df2_copy['MMSE'] = df2['MMSE']\n",
    "\n",
    "# Concatenate df3 and df2_copy\n",
    "df3 = pd.concat([df3, df2_copy], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91efc993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 550 entries, 0 to 549\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   cookie test   550 non-null    object \n",
      " 1   dementia      550 non-null    int64  \n",
      " 2   Error Type 1  550 non-null    int64  \n",
      " 3   Error Type 2  550 non-null    int64  \n",
      " 4   File          550 non-null    object \n",
      " 5   Age           460 non-null    float64\n",
      " 6   Gender        548 non-null    object \n",
      " 7   MMSE          456 non-null    float64\n",
      "dtypes: float64(2), int64(3), object(3)\n",
      "memory usage: 34.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "096d2be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cookie test</th>\n",
       "      <th>dementia</th>\n",
       "      <th>Error Type 1</th>\n",
       "      <th>Error Type 2</th>\n",
       "      <th>File</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>MMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mhm . \u00153609_4282\u0015there's a young boy &amp;uh going...</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>001-2.cha</td>\n",
       "      <td>59.0</td>\n",
       "      <td>male</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>here's a cookie jar . \u00150_8778\u0015and the lid is o...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>003-0.cha</td>\n",
       "      <td>56.0</td>\n",
       "      <td>male</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the boy is slipping off the stool . \u00156536_1030...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>005-0.cha</td>\n",
       "      <td>53.0</td>\n",
       "      <td>male</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>okay he's fallin(g) off a chair [: stool] [* s...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>005-2.cha</td>\n",
       "      <td>55.0</td>\n",
       "      <td>male</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xxx . [+ exc] \u00152744_6330\u0015can I look at it and ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>007-1.cha</td>\n",
       "      <td>73.0</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>well the girl is telling the boy to get the co...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>007-3.cha</td>\n",
       "      <td>75.0</td>\n",
       "      <td>female</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>oh boy . [+ exc] \u00152194_3657\u0015wowie the boy's go...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>010-0.cha</td>\n",
       "      <td>66.0</td>\n",
       "      <td>male</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>oh boy . [+ exc] \u00151929_3209\u0015alright . [+ exc] ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>010-1.cha</td>\n",
       "      <td>67.0</td>\n",
       "      <td>male</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>what's happening there ? [+ exc] \u00151546_2827\u0015oh...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>010-2.cha</td>\n",
       "      <td>68.0</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>&lt;well the kid&gt; [//] the girl's laughin(g) at h...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>010-3.cha</td>\n",
       "      <td>69.0</td>\n",
       "      <td>male</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         cookie test  dementia  Error Type 1  \\\n",
       "0  mhm . \u00153609_4282\u0015there's a young boy &uh going...         1             7   \n",
       "1  here's a cookie jar . \u00150_8778\u0015and the lid is o...         1             5   \n",
       "2  the boy is slipping off the stool . \u00156536_1030...         1             0   \n",
       "3  okay he's fallin(g) off a chair [: stool] [* s...         1             1   \n",
       "4  xxx . [+ exc] \u00152744_6330\u0015can I look at it and ...         1             1   \n",
       "5  well the girl is telling the boy to get the co...         1             1   \n",
       "6  oh boy . [+ exc] \u00152194_3657\u0015wowie the boy's go...         1             3   \n",
       "7  oh boy . [+ exc] \u00151929_3209\u0015alright . [+ exc] ...         1             0   \n",
       "8  what's happening there ? [+ exc] \u00151546_2827\u0015oh...         1             4   \n",
       "9  <well the kid> [//] the girl's laughin(g) at h...         1             1   \n",
       "\n",
       "   Error Type 2       File   Age  Gender  MMSE  \n",
       "0             6  001-2.cha  59.0    male  11.0  \n",
       "1             4  003-0.cha  56.0    male  20.0  \n",
       "2             2  005-0.cha  53.0    male  23.0  \n",
       "3             0  005-2.cha  55.0    male  19.0  \n",
       "4             1  007-1.cha  73.0  female  19.0  \n",
       "5             0  007-3.cha  75.0  female  15.0  \n",
       "6             4  010-0.cha  66.0    male  20.0  \n",
       "7             3  010-1.cha  67.0    male  21.0  \n",
       "8             3  010-2.cha  68.0    male  26.0  \n",
       "9             3  010-3.cha  69.0    male  19.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ff4848b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "text = [] \n",
    "cleaned_text = []\n",
    "\n",
    "for element in df3['cookie test']:\n",
    "    # Remove the \"xxx\" at the beginning of the text\n",
    "    f_text = re.sub(r'^xxx\\s*', '', element)\n",
    "    \n",
    "    # Remove words within parentheses (including the parentheses)\n",
    "    f_text = re.sub(r'\\([^)]*\\)', '', f_text)\n",
    "    \n",
    "    # Replace consecutive spaces with a single space, remove non-alphabetic characters, and convert to lowercase\n",
    "    f_text = re.sub(r\"\\s+\", \" \", f_text.strip())\n",
    "    f_text = re.sub(r\"[^a-záéíóúüñ' ]\", \" \", f_text.lower())\n",
    "    \n",
    "    # Remove the word \"exc\" when it appears as a standalone word\n",
    "    f_text = re.sub(r\"\\bexc\\b\", \"\", f_text)\n",
    "    \n",
    "    # Split the text into words\n",
    "    docto_text = f_text.split(\" \")\n",
    "    \n",
    "    # Reconstruct the cleaned text while removing extra spaces\n",
    "    cleaned_text.append(\" \".join(docto_text))\n",
    "    \n",
    "    # Create a list of words without stopwords\n",
    "    x = [word for word in docto_text if word not in stop_words]\n",
    "    text.append(x)\n",
    "\n",
    "# Add the 'cleaned_text' column to your DataFrame\n",
    "#df3['cleaned_text'] = cleaned_text\n",
    "df3['final_text']= text\n",
    "vocabulary=[]\n",
    "voc_siz=[]\n",
    "lex_voc=[]\n",
    "for i in range(len(text)):\n",
    "    voc=list(set(text[i]))\n",
    "    l_t=len(text[i])\n",
    "    siz_voc=len(voc)\n",
    "    vocabulary.append(voc)\n",
    "    voc_siz.append(siz_voc)\n",
    "    lex_voc.append(siz_voc/l_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59c1dcd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'her', 'until', 'isn', 'while', 'mustn', 'weren', \"aren't\", 'each', 'aren', 'again', \"you've\", 've', 'wouldn', \"doesn't\", \"you'd\", 'are', 'when', \"she's\", 'than', 'out', 'shouldn', \"haven't\", 'its', 'he', \"that'll\", 'herself', 'the', 'through', \"shouldn't\", 'as', 'won', 'what', 'during', 'very', 'them', 'too', 'for', 'y', 'hasn', 'himself', \"mightn't\", 'didn', \"you're\", 'can', 't', 'wasn', 'hadn', 'be', 'haven', 'if', \"shan't\", 'their', \"hadn't\", 'ours', 'then', 'myself', 'below', \"won't\", \"didn't\", 'above', 'now', 're', \"isn't\", 'doesn', 'am', 'itself', 'both', 'in', 'there', 'shan', 'few', \"couldn't\", 'these', 'i', 'were', 'under', 'just', 'only', 'such', 'being', 'was', 'has', 'my', 'at', 'not', 'yourself', 'up', 'mightn', 'by', 'with', 'does', 'is', 'between', 'have', 'theirs', 'whom', 'because', 'yourselves', 'further', 'nor', \"don't\", 'll', 'here', 'from', 'having', 'down', 'once', 'same', 'd', 'no', 'it', 'before', 'own', 'doing', \"it's\", 'm', 'they', 'off', 'ma', 'that', 'me', 'after', 'should', 'yours', 'any', 'you', 'we', 'your', 'a', 's', 'couldn', 'ain', 'hers', 'our', 'why', 'other', 'and', \"you'll\", 'him', 'those', 'ourselves', \"hasn't\", 'of', 'do', 'which', 'or', 'on', \"mustn't\", 'about', 'this', \"wasn't\", 'who', 'did', 'but', 'against', \"wouldn't\", 'most', 'to', 'how', 'where', 'his', 'will', 'so', \"should've\", \"needn't\", 'themselves', 'into', 'all', 'don', 'she', 'over', 'o', 'needn', 'some', 'been', \"weren't\", 'more', 'an', 'had'}\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00a70b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned text ['mhm', '', '', '', '', '', '', '', '', '', '', '', '', '', \"there's\", 'young', 'boy', '', 'uh', 'going', 'cookie', 'jar', '', '', '', '', '', '', '', '', '', '', '', '', '', '', \"there's\", '', '', '', '', '', 'lit', 'girl', '', '', '', '', '', 'young', 'girl', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', \"i'm\", 'sayin', \"he's\", 'boy', 'cause', '', '', '', '', '', '', '', '', 'hard', 'uh', \"he's\", '', '', '', '', \"he's\", '', 'c', '', 'cookie', 'jar', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', \"there's\", '', 'stool', 'already', 'starting', 'toand', 'water', 'sink', '', 'uh', '', 'ev', 'overflowing', 'sink', 'hm', '', '', '', '', 'know', '', '', '', '', '', 'hickey', '', '', '', '', '', 'uh', 'like', '', '', 'uh', 'wife', '', 'g', 'mean', '', '', '', '', '', '', '', 'uh', '', '', '', '', 'motherand', '', '', '', '', '', '', 'uh', '', 'w', '', 'uh', '', 'h', '', '', '', '', '', 'uh', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'oh', '', 'uh', '', '', '', '', \"can't\", 'think', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'uh', 'tryin', 'wipe', '', '', '', '', '', 'uh', 'wipe', 'dishes', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'oh', '', 'stop', 'water', 'going', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "print(\"cleaned text\",df3['final_text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3ee2770",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# keys_dem=pd.read_csv(r\"C:\\Users\\marco\\OneDrive\\Documents\\Ijtsi\\THESIS\\key.csv\",delimiter=',')  \n",
    "# key_words=pd.DataFrame(keys_dem)\n",
    "\n",
    "# # In[]\n",
    "# count_key = []\n",
    "\n",
    "# for cleaned_text in df3['final_text']:\n",
    "#     h = 0\n",
    "    \n",
    "#     for word in cleaned_text.split():\n",
    "#         m = key_words['Keys'].str.fullmatch(word)\n",
    "#         h += m.sum()\n",
    "    \n",
    "#     count_key.append(h)\n",
    "\n",
    "keys_dem = pd.read_csv(r\"C:\\Users\\marco\\OneDrive\\Documents\\Ijtsi\\THESIS\\key.csv\", delimiter=',')  \n",
    "key_words = pd.DataFrame(keys_dem)\n",
    "\n",
    "count_key = []\n",
    "\n",
    "for word_list in df3['final_text']:\n",
    "    h = 0\n",
    "    \n",
    "    for word in word_list:\n",
    "        m = key_words['Keys'].str.fullmatch(word)\n",
    "        h += m.sum()\n",
    "    \n",
    "    count_key.append(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "601ca786",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "corpus_embeddings = model.encode(df3['final_text'])\n",
    "#pairwise_similarities=cosine_similarity(corpus_embeddings)\n",
    "#sim=pairwise_similarities[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bca2ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#all-MiniLM-L6-v2\n",
    "embedder = SentenceTransformer('all-mpnet-base-v2')\n",
    "final_storage=[]\n",
    "for element in df3['final_text']:\n",
    "    corpus= element\n",
    "    corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)\n",
    "    corpus_storage=[]\n",
    "    \n",
    "    # Query sentences:\n",
    "    queries = ['children stealing cookies', 'woman doing dishes', 'girl reaching for a cookie','woman not noticing','boy on stool','sink overflowing','stool falling']\n",
    "    top_k = min(1, len(corpus))\n",
    "    for query in queries:\n",
    "        query_embedding = embedder.encode(query, convert_to_tensor=True)\n",
    "    \n",
    "        # We use cosine-similarity and torch.topk to find the highest 5 scores\n",
    "        cos_scores = util.pytorch_cos_sim(query_embedding, corpus_embeddings)[0]\n",
    "        top_results = torch.topk(cos_scores, k=top_k)\n",
    "        for score, idx in zip(top_results[0], top_results[1]):\n",
    "            results=float(score)\n",
    "            corpus_storage.append(results)\n",
    "    final_storage.append(corpus_storage)\n",
    "# In[]\n",
    "main_1=[]\n",
    "main_2=[]\n",
    "main_3=[]\n",
    "main_4=[]\n",
    "main_5=[]\n",
    "main_6=[]\n",
    "main_7=[]\n",
    "\n",
    "for i in range(len(final_storage)):\n",
    "    m1=final_storage[i][0]\n",
    "    main_1.append(m1)\n",
    "    m2=final_storage[i][1]\n",
    "    main_2.append(m2)\n",
    "    m3=final_storage[i][2]\n",
    "    main_3.append(m3)\n",
    "    m4=final_storage[i][3]\n",
    "    main_4.append(m4)\n",
    "    m5=final_storage[i][4]\n",
    "    main_5.append(m5)\n",
    "    m6=final_storage[i][5]\n",
    "    main_6.append(m6)\n",
    "    m7=final_storage[i][6]\n",
    "    main_7.append(m7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "feb1b0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_main=[]\n",
    "for i in range(len(final_storage)):\n",
    "    main_used=[]\n",
    "    for element in final_storage[i]:\n",
    "        x=0\n",
    "        if element>=0.50:\n",
    "            x=1\n",
    "        main_used.append(x)\n",
    "    count=sum(main_used)\n",
    "    count_main.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a12fb090",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (906036687.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\marco\\AppData\\Local\\Temp\\ipykernel_18980\\906036687.py\"\u001b[1;36m, line \u001b[1;32m8\u001b[0m\n\u001b[1;33m    0vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvg    pd.DataFrame({\"main idea 4\": main_4}),\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "data_frames_to_concat = [\n",
    "    pd.DataFrame({\"vocabulary\": voc_siz}),\n",
    "    pd.DataFrame({\"lex_div\": lex_voc}),\n",
    "    pd.DataFrame({\"key_words\": count_key}),\n",
    "    pd.DataFrame({\"main idea 1\": main_1}),\n",
    "    pd.DataFrame({\"main idea 2\": main_2}),\n",
    "    pd.DataFrame({\"main idea 3\": main_3}),\n",
    "    pd.DataFrame({\"main idea 4\": main_4}),\n",
    "    pd.DataFrame({\"main idea 5\": main_5}),\n",
    "    pd.DataFrame({\"main idea 6\": main_6}),\n",
    "    pd.DataFrame({\"main idea 7\": main_7}),\n",
    "    pd.DataFrame({\"count main used\": count_main}),\n",
    "   # pd.DataFrame({\"sim\": sim})\n",
    "]\n",
    "\n",
    "# Concatenate DataFrames to df3 column-wise\n",
    "df3 = pd.concat([df3] + data_frames_to_concat, axis=1)\n",
    "#df3 = df3.drop([\"cookie test\", \"final_text\"], axis=1)\n",
    "\n",
    "# Save the updated df3 to a CSV file or perform any further operations\n",
    "df3.to_csv('final_NLP.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "730ded1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the first CSV file\n",
    "file1 = pd.read_csv('final_NLP.csv')\n",
    "\n",
    "# Load the second CSV file\n",
    "file2 = pd.read_csv('Pitt-data.csv', header=2)\n",
    "\n",
    "# Extract the id from the \"File\" column in file1\n",
    "file1['id'] = file1['File'].str.extract(r'(\\d+)-\\d+\\.cha').astype(int)\n",
    "\n",
    "# Merge only the \"id,\" \"race,\" and \"educ\" columns from file2\n",
    "merged_df = file1.merge(file2[['id', 'educ']], on='id', how='left')\n",
    "\n",
    "# Rename the columns as per your requirement\n",
    "merged_df.rename(columns={'educ': 'Education level'}, inplace=True)\n",
    "\n",
    "# Save the merged dataframe to a new CSV file\n",
    "merged_df.to_csv('merged_file.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4453e7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
